{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2XD9VUW4ChbSXj5VSwxtV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhigoel25/StudentPerformancePredictionModel/blob/main/StudentPerformancePrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SM9mDge5PK8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "print(uploaded)"
      ],
      "metadata": {
        "id": "5GLI2jfw6hEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(io.BytesIO(uploaded[\"Student_performance_data _.csv\"]))\n",
        "df"
      ],
      "metadata": {
        "id": "pb1GDnO86rYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns = ['StudentID'])\n",
        "df"
      ],
      "metadata": {
        "id": "-CeTb3ey7QqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_encode = ['Ethnicity', 'ParentalEducation']\n",
        "df = pd.get_dummies(df, columns = columns_to_encode).astype(float)\n",
        "df"
      ],
      "metadata": {
        "id": "Trb1_gXT9WDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grades = df['GradeClass']  #Shows that better GPA corresponds to better grade class (grade class of 0 is best grade class)\n",
        "gpa = df['GPA']\n",
        "\n",
        "plt.scatter(gpa, grades)\n",
        "plt.xlabel('GPA')\n",
        "plt.ylabel('Grade')\n",
        "plt.title('Scatter Plot of GPA vs Grade Class for Students')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wHw1jGoLKdM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['GradeClass']\n",
        "x = df.drop(columns = ['GradeClass']).values\n",
        "\n",
        "print(\"x.shape\", x.shape)\n",
        "print(\"y.shape\", y.shape)\n",
        "\n",
        "print(np.isnan(x).any())\n",
        "print(np.isnan(y).any())\n",
        "\n",
        "scaler = StandardScaler()\n",
        "columns_to_standardize = ['Age', 'StudyTimeWeekly', 'Absences']\n",
        "df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
        "df"
      ],
      "metadata": {
        "id": "uAU8waMh9yt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTransform():\n",
        "    def __call__(self, sample):\n",
        "        x, y = sample\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "        y = torch.tensor(y, dtype=torch.float32)\n",
        "        return x, y\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x, y, transform):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): # return how many rows are in the dataset\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx): # return a single example from the dataset\n",
        "        sample = (self.x[idx], self.y[idx])\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "metadata": {
        "id": "0e-DZkvA-PGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = CustomTransform()\n",
        "dataset = CustomDataset(x, y, transform=transform)\n"
      ],
      "metadata": {
        "id": "dan1DaFM-2xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.7 * len(dataset))\n",
        "dev_size = int(0.15 * len(dataset))\n",
        "test_size = len(dataset) - dev_size - train_size\n",
        "\n",
        "print(train_size)\n",
        "print(dev_size)\n",
        "print(test_size)\n",
        "\n",
        "train_dataset, dev_dataset, test_dataset = random_split(dataset, [train_size, dev_size, test_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=30, shuffle=True) # training set\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=30, shuffle=False) # validation set\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=30, shuffle=False)\n",
        "\n",
        "\n",
        "i=0\n",
        "for batch_x, batch_y in train_dataloader:\n",
        "    print(\"x.shape = {}\".format(batch_x.shape), \"y.shape = {}\".format(batch_y.shape))\n",
        "    print(batch_x)\n",
        "    print(batch_y)\n",
        "    i += 1\n",
        "    if i > 5:\n",
        "      break"
      ],
      "metadata": {
        "id": "4rwh-q27--OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StudentPerformanceNN(nn.Module):\n",
        "  def __init__(self, input_size):\n",
        "    super(StudentPerformanceNN, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_size, 128)\n",
        "    self.bn1 = nn.BatchNorm1d(128)\n",
        "    self.fc2 = nn.Linear(128, 64)\n",
        "    self.bn2 = nn.BatchNorm1d(64)\n",
        "    self.fc3 = nn.Linear(64, 32)\n",
        "    self.bn3 = nn.BatchNorm1d(32)\n",
        "    self.fc4 = nn.Linear(32, 5)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.fc1(x)\n",
        "      x = self.bn1(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.fc2(x)\n",
        "      x = self.bn2(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.fc3(x)\n",
        "      x = self.bn3(x)\n",
        "      x = self.relu(x)\n",
        "      x = self.fc4(x)\n",
        "      x = self.softmax(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "uLeLj4jVA1oZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = x.shape[1]\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "train_loss =  []\n",
        "val_loss = []\n",
        "model = StudentPerformanceNN(input_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0009)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  correct_train = 0\n",
        "  total_train = 0\n",
        "  i = 0\n",
        "  for batch_x, batch_y in train_dataloader:\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(batch_x)\n",
        "    batch_y = batch_y.view(-1, 1) # reshaping to conform to outputs prediction shape\n",
        "    batch_y = batch_y.squeeze(1).long() # removing the extra dimension\n",
        "    #print(batch_y)\n",
        "    #print(outputs)\n",
        "    #print(batch_y)\n",
        "    #print(f'Outputs shape: {outputs.shape}')  # Should be [batch_size, num_classes]\n",
        "    #print(f'Targets shape: {batch_y.shape}')  # Should be [batch_size]\n",
        "    loss = criterion(outputs, batch_y)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    _, predicted = torch.max(outputs.data, 1) # once you make predictions, torch.max takes the max probability for each example and selects the corresponding class\n",
        "    total_train += batch_y.size(0)\n",
        "    correct_train += (predicted == batch_y).sum().item() # how many predictions were correct\n",
        "    i +=1\n",
        "    if i % 25 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
        "  train_accuracy = 100 * correct_train / total_train\n",
        "  train_accuracies.append(train_accuracy)\n",
        "  train_loss.append(loss.item())\n",
        "  print(f\"Train Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "  #Validation Loop\n",
        "  for batch_x, batch_y in dev_dataloader:\n",
        "    model.eval()\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(batch_x)\n",
        "      batch_y = batch_y.view(-1, 1) # reshaping to conform to outputs prediction shape\n",
        "      batch_y = batch_y.squeeze(1).long() # removing the extra dimension\n",
        "      loss = criterion(outputs, batch_y)\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1) # once you make predictions, torch.max takes the max probability for each example and selects the corresponding class\n",
        "    total_val += batch_y.size(0)\n",
        "    correct_val += (predicted == batch_y).sum().item() # how many predictions were correct\n",
        "    i +=1\n",
        "    if i % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dev_dataloader)}], Loss: {loss.item():.4f}')\n",
        "  val_accuracy = 100 * correct_val / total_val\n",
        "  val_accuracies.append(val_accuracy)\n",
        "  val_loss.append(loss.item())\n",
        "\n",
        "  print(f\"Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs+1), train_accuracies, label='Train Accuracy')\n",
        "plt.plot(range(1, num_epochs+1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Train and Validation Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, num_epochs+1), train_loss, label='Train Loss')\n",
        "plt.plot(range(1, num_epochs+1), val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cross Entropy Loss')\n",
        "plt.title('Train and Validation Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2imzfKgACLJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_curve\n",
        "from sklearn.preprocessing import label_binarize"
      ],
      "metadata": {
        "id": "RhB_lguyP6M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Loop\n",
        "test_accuracies = []\n",
        "model.eval()\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "test_loss = 0.0\n",
        "num_classes = 5\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch_x, batch_y in test_dataloader:\n",
        "    outputs = model(batch_x)\n",
        "    batch_y = batch_y.view(-1, 1) # reshaping to conform to outputs prediction shape\n",
        "    batch_y = batch_y.squeeze(1).long() # removing the extra dimension\n",
        "    test_loss = criterion(outputs, batch_y)\n",
        "\n",
        "\n",
        "    _, predicted = torch.max(outputs.data, 1) # once you make predictions, torch.max takes the max probability for each example and selects the corresponding class\n",
        "    y_prediction = predicted.cpu().numpy()\n",
        "    y_target = batch_y.cpu().numpy()\n",
        "    total_test += batch_y.size(0)\n",
        "    print(f'Output Prediction: {predicted}')\n",
        "    print(f'Target Prediction: {batch_y}')\n",
        "    correct_test += (predicted == batch_y).sum().item() # how many predictions were correct\n",
        "\n",
        "    class_accuracies = []\n",
        "    for i in range(num_classes):\n",
        "      class_indices = [j for j, x in enumerate(y_target) if x == i]\n",
        "      class_accuracy = accuracy_score([y_target[j] for j in class_indices], [y_prediction[j] for j in class_indices])\n",
        "      class_accuracies.append(class_accuracy)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(num_classes), class_accuracies)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Class-wise Accuracy')\n",
        "plt.xticks(range(num_classes))\n",
        "plt.show()\n",
        "\n",
        "print(f'Epoch [{epoch+1}/{num_epochs}], Test Loss: {test_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "'''\n",
        "    cm = confusion_matrix(y_target, y_prediction)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "    test_loss /= len(test_dataloader)\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "    test_accuracies.append(test_accuracy)\n",
        "'''"
      ],
      "metadata": {
        "id": "jby0Gau-EWQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the accuracies\n",
        "train_accuracy = train_accuracies[-1]/100\n",
        "validation_accuracy = val_accuracies[-1]/100\n",
        "test_accuracy = test_accuracies[-1]/100\n",
        "\n",
        "# Define the labels and the values\n",
        "labels = ['Train Accuracy', 'Validation Accuracy', 'Test Accuracy']\n",
        "accuracies = [train_accuracy, validation_accuracy, test_accuracy]\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(labels, accuracies, color=['blue', 'green', 'red'])\n",
        "plt.ylim(0, 1)\n",
        "plt.xlabel('Accuracy Type')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Train, Validation, and Test Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dzgdjG4SIH27"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}